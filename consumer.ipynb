{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries & Ensure tables are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ensured 'redset_main' table exists.\n",
      "✅ Ensured 'anomalies' table exists.\n",
      "✅ Ensured 'top_k' materialized view exists.\n",
      "✅ Ensured 'top_k_queries_per_day' materialized view exists.\n",
      "✅ Ensured 'hit_rate' materialized view exists.\n",
      "✅ Materialized view 'compile_time_vs_num_joins' created successfully.\n",
      "✅ Unique index 'idx_compile_time_vs_num_joins' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from confluent_kafka import Consumer, Producer\n",
    "import json\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import psycopg2\n",
    "from pyspark.sql import Row\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Database connection details - muhid\n",
    "# DB_NAME = \"redset\"\n",
    "# DB_USER = \"postgres\"\n",
    "# DB_PASS = \"admin123\"\n",
    "# DB_HOST = \"localhost\"\n",
    "# DB_PORT = \"5432\"\n",
    "\n",
    "# Database connection details - goutham\n",
    "DB_NAME = \"de_project_main\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASS = \"postgres16\"\n",
    "# DB_HOST = \"192.168.66.138\"  # muhid hotspot\n",
    "# DB_HOST = \"192.168.127.138\"  # sagnik hotspot\n",
    "DB_HOST = \"192.168.7.138\"  # sagnik hotspot\n",
    "DB_PORT = \"5432\"\n",
    "\n",
    "# Expected Schema (Pandas-compatible)\n",
    "expected_schema = {\n",
    "    \"user_id\": \"int\",\n",
    "    \"query_id\": \"int\",  # Added query_id to expected schema\n",
    "    \"arrival_timestamp\": \"datetime64[ns]\",\n",
    "    \"compile_duration_ms\": \"float\",\n",
    "    \"execution_duration_ms\": \"int\",\n",
    "    \"was_cached\": \"int\",\n",
    "    \"query_type\": \"str\",\n",
    "    \"read_table_ids\": \"str\",\n",
    "    \"write_table_ids\": \"str\",\n",
    "    \"num_joins\": \"int\",\n",
    "    \"num_scans\": \"int\"\n",
    "}\n",
    "\n",
    "# Valid Query Types\n",
    "valid_query_types = {\"select\", \"insert\", \"delete\", \"other\", \"analyze\", \"unload\", \"update\", \"copy\", \"ctas\", \"vacuum\"}\n",
    "\n",
    "# Invalid Table IDs\n",
    "invalid_table_ids = {\"00\", \"000\", \"0000\"}\n",
    "\n",
    "# Set Maximum Values for Optimization Alert\n",
    "# MAX_MBYTES_SCANNED = 100000\n",
    "\n",
    "# Create PostgreSQL Connection\n",
    "engine = create_engine(f'postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}')\n",
    "\n",
    "# Function to ensure `redset_main` table exists\n",
    "def ensure_master_table():\n",
    "    create_table_query = \"\"\"\n",
    "    -- DROP TABLE IF EXISTS public.\"redset_main\" CASCADE;\n",
    "    CREATE TABLE IF NOT EXISTS \"redset_main\" (\n",
    "        user_id INT,\n",
    "        query_id INT,\n",
    "        arrival_timestamp TIMESTAMP,\n",
    "        compile_duration_ms FLOAT,\n",
    "        execution_duration_ms FLOAT,\n",
    "        was_cached INT,\n",
    "        query_type TEXT,\n",
    "        read_table_ids TEXT,\n",
    "        write_table_ids TEXT,\n",
    "        num_joins INT,\n",
    "        num_scans INT\n",
    "    ) PARTITION BY RANGE (arrival_timestamp);\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(create_table_query))\n",
    "        conn.commit()\n",
    "    print(\"✅ Ensured 'redset_main' table exists.\")\n",
    "\n",
    "# ✅ Function to ensure `anomalies` table exists\n",
    "def ensure_anomalies_table():\n",
    "    create_table_query = \"\"\"\n",
    "    -- DROP TABLE IF EXISTS public.\"anomalies\" CASCADE;\n",
    "    CREATE TABLE IF NOT EXISTS anomalies (\n",
    "        arrival_timestamp TIMESTAMP NOT NULL,\n",
    "        query_id INT NOT NULL,  -- Added query_id column\n",
    "        anomaly_description TEXT NOT NULL\n",
    "    );\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(create_table_query))\n",
    "        conn.commit()\n",
    "    print(\"✅ Ensured 'anomalies' table exists.\")\n",
    "\n",
    "# ✅ Function to ensure `stream_summary` table exists\n",
    "def stream_summary_table():\n",
    "    create_table_query = \"\"\"\n",
    "    -- DROP TABLE IF EXISTS public.\"stream_summary\" CASCADE;\n",
    "    CREATE TABLE IF NOT EXISTS stream_summary (\n",
    "        table_id INT PRIMARY KEY,\n",
    "        count BIGINT DEFAULT 1\n",
    "    );\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(create_table_query))\n",
    "        conn.commit()\n",
    "    print(\"✅ Ensured 'stream_summary' table exists.\")\n",
    "\n",
    "# ✅ Function to ensure `global_counts` table exists\n",
    "def global_counts_table():\n",
    "    create_table_query = \"\"\"\n",
    "    -- DROP TABLE IF EXISTS public.\"global_counts\" CASCADE;\n",
    "    CREATE TABLE IF NOT EXISTS global_counts (\n",
    "        key TEXT PRIMARY KEY,\n",
    "        total_elements_seen BIGINT DEFAULT 0\n",
    "    );\n",
    "    -- Initialize total_elements_seen if not exists\n",
    "    INSERT INTO global_counts (key, total_elements_seen)\n",
    "    VALUES ('total_tables', 0)\n",
    "    ON CONFLICT (key) DO NOTHING;\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(create_table_query))\n",
    "        conn.commit()\n",
    "    print(\"✅ Ensured 'stream_summary' table exists.\")\n",
    "\n",
    "\n",
    "def top_k_tables_per_day_materialized_view():\n",
    "    create_view_query = \"\"\"\n",
    "    -- View: public.top_k_tables_per_day\n",
    "\n",
    "    -- DROP MATERIALIZED VIEW IF EXISTS public.top_k_tables_per_day CASCADE;\n",
    "\n",
    "    CREATE MATERIALIZED VIEW IF NOT EXISTS public.top_k_tables_per_day\n",
    "    TABLESPACE pg_default\n",
    "    AS\n",
    "    WITH table_usage AS (\n",
    "            SELECT \"redset_main\".arrival_timestamp,\n",
    "                date_trunc('day'::text, \"redset_main\".arrival_timestamp) AS day,\n",
    "                unnest(string_to_array(\"redset_main\".read_table_ids, ','::text)) AS table_id,\n",
    "                \"redset_main\".query_type,\n",
    "                \"redset_main\".user_id\n",
    "            FROM \"redset_main\"\n",
    "            UNION ALL\n",
    "            SELECT \"redset_main\".arrival_timestamp,\n",
    "                date_trunc('day'::text, \"redset_main\".arrival_timestamp) AS day,\n",
    "                unnest(string_to_array(\"redset_main\".write_table_ids, ','::text)) AS table_id,\n",
    "                \"redset_main\".query_type,\n",
    "                \"redset_main\".user_id\n",
    "            FROM \"redset_main\"\n",
    "            ), table_count AS (\n",
    "            SELECT table_usage_1.day,\n",
    "                table_usage_1.table_id,\n",
    "                table_usage_1.query_type,\n",
    "                table_usage_1.user_id,\n",
    "                count(*) AS count\n",
    "            FROM table_usage table_usage_1\n",
    "            GROUP BY table_usage_1.day, table_usage_1.table_id, table_usage_1.query_type, table_usage_1.user_id\n",
    "            ), total_count AS (\n",
    "            SELECT table_count.day,\n",
    "                sum(table_count.count) AS total\n",
    "            FROM table_count\n",
    "            GROUP BY table_count.day\n",
    "            ), overall_total AS (\n",
    "            SELECT sum(total_count.total) AS overall_total\n",
    "            FROM total_count\n",
    "            ), table_percentage AS (\n",
    "            SELECT table_count.day,\n",
    "                table_count.table_id,\n",
    "                table_count.query_type,\n",
    "                table_count.user_id,\n",
    "                table_count.count,\n",
    "                table_count.count::double precision / (( SELECT total_count.total\n",
    "                    FROM total_count\n",
    "                    WHERE total_count.day = table_count.day))::double precision * 100::double precision AS percentage,\n",
    "                table_count.count::double precision / (( SELECT overall_total.overall_total\n",
    "                    FROM overall_total))::double precision * 100::double precision AS overall_percentage\n",
    "            FROM table_count\n",
    "            )\n",
    "    SELECT day,\n",
    "        table_id,\n",
    "        query_type,\n",
    "        user_id,\n",
    "        count,\n",
    "        percentage,\n",
    "        overall_percentage\n",
    "    FROM table_percentage\n",
    "    WITH DATA;\n",
    "\n",
    "    ALTER TABLE IF EXISTS public.top_k_tables_per_day\n",
    "        OWNER TO postgres;\n",
    "\n",
    "\n",
    "    CREATE UNIQUE INDEX idx_top_k_tables_per_day\n",
    "        ON public.top_k_tables_per_day USING btree\n",
    "        (day, table_id COLLATE pg_catalog.\"default\", query_type COLLATE pg_catalog.\"default\", user_id)\n",
    "        TABLESPACE pg_default;\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(create_view_query))\n",
    "        conn.commit()\n",
    "    print(\"✅ Ensured 'top_k' materialized view exists.\")\n",
    "\n",
    "def hit_rate_per_day_materialized_view():\n",
    "    create_view_query = \"\"\"\n",
    "    -- Drop the materialized view if it exists\n",
    "    -- DROP MATERIALIZED VIEW IF EXISTS public.hit_rate_per_day CASCADE;\n",
    "\n",
    "    -- Create the materialized view\n",
    "    CREATE MATERIALIZED VIEW IF NOT EXISTS public.hit_rate_per_day\n",
    "    TABLESPACE pg_default\n",
    "    AS\n",
    "    WITH daily_stats AS (\n",
    "        SELECT \n",
    "            date_trunc('day', \"redset_main\".arrival_timestamp) AS day,\n",
    "            \"redset_main\".query_type,\n",
    "            \"redset_main\".user_id,\n",
    "            COUNT(*) FILTER (WHERE \"redset_main\".was_cached = 1) AS was_cached_count,\n",
    "            COUNT(*) AS total_count\n",
    "        FROM \n",
    "            \"redset_main\"\n",
    "        GROUP BY \n",
    "            date_trunc('day', \"redset_main\".arrival_timestamp),\n",
    "            \"redset_main\".query_type,\n",
    "            \"redset_main\".user_id\n",
    "    ),\n",
    "    daily_totals AS (\n",
    "        SELECT \n",
    "            day,\n",
    "            SUM(total_count) AS total_queries\n",
    "        FROM \n",
    "            daily_stats\n",
    "        GROUP BY \n",
    "            day\n",
    "    )\n",
    "    SELECT \n",
    "        ds.day,\n",
    "        ds.query_type,\n",
    "        ds.user_id,\n",
    "        ds.was_cached_count,\n",
    "        dt.total_queries,\n",
    "        (ds.was_cached_count::double precision / dt.total_queries::double precision) * 100 AS hit_rate_per_day\n",
    "    FROM \n",
    "        daily_stats ds\n",
    "    JOIN \n",
    "        daily_totals dt\n",
    "    ON \n",
    "        ds.day = dt.day\n",
    "    WITH DATA;\n",
    "\n",
    "    -- Create a unique index on the materialized view\n",
    "    CREATE UNIQUE INDEX idx_hit_rate_per_day ON public.hit_rate_per_day (day, query_type, user_id);\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(create_view_query))\n",
    "        conn.commit()\n",
    "    print(\"✅ Ensured 'hit_rate' materialized view exists.\")\n",
    "\n",
    "def top_k_queries_per_day_materialized_view():\n",
    "    create_view_query = \"\"\"\n",
    "    -- Drop the materialized view if it exists\n",
    "    -- DROP MATERIALIZED VIEW IF EXISTS public.top_k_queries_per_day CASCADE;\n",
    "\n",
    "    -- Create the materialized view\n",
    "    CREATE MATERIALIZED VIEW IF NOT EXISTS public.top_k_queries_per_day\n",
    "    TABLESPACE pg_default\n",
    "    AS\n",
    "    WITH query_usage AS (\n",
    "        SELECT \n",
    "            public.redset_main.arrival_timestamp,\n",
    "            date_trunc('day', public.redset_main.arrival_timestamp) AS day,\n",
    "            unnest(string_to_array(public.redset_main.query_type, ',')) AS query_type,\n",
    "            public.redset_main.user_id\n",
    "        FROM public.redset_main\n",
    "    ),\n",
    "    query_count AS (\n",
    "        SELECT \n",
    "            query_usage.day,\n",
    "            query_usage.query_type,\n",
    "            query_usage.user_id,\n",
    "            COUNT(*) AS count\n",
    "        FROM query_usage\n",
    "        GROUP BY query_usage.day, query_usage.query_type, query_usage.user_id\n",
    "    ),\n",
    "    total_queries_per_day AS (\n",
    "        SELECT \n",
    "            day,\n",
    "            SUM(count) AS total_queries\n",
    "        FROM query_count\n",
    "        GROUP BY day\n",
    "    ),\n",
    "    overall_total_queries AS (\n",
    "        SELECT SUM(total_queries) AS overall_total\n",
    "        FROM total_queries_per_day\n",
    "    ),\n",
    "    query_percentage AS (\n",
    "        SELECT \n",
    "            query_count.day,\n",
    "            query_count.query_type,\n",
    "            query_count.user_id,\n",
    "            query_count.count,\n",
    "            (query_count.count::double precision / \n",
    "                (SELECT total_queries_per_day.total_queries \n",
    "                FROM total_queries_per_day \n",
    "                WHERE total_queries_per_day.day = query_count.day)) * 100 AS daily_percentage,\n",
    "            (query_count.count::double precision / \n",
    "                (SELECT overall_total_queries.overall_total FROM overall_total_queries)) * 100 AS overall_percentage\n",
    "        FROM query_count\n",
    "    )\n",
    "    SELECT \n",
    "        query_percentage.day,\n",
    "        query_percentage.query_type,\n",
    "        query_percentage.user_id,\n",
    "        query_percentage.count,\n",
    "        query_percentage.daily_percentage,\n",
    "        query_percentage.overall_percentage\n",
    "    FROM query_percentage\n",
    "    ORDER BY query_percentage.day DESC, query_percentage.count DESC\n",
    "    WITH DATA;\n",
    "\n",
    "    -- Create a unique index on the materialized view for faster querying\n",
    "    CREATE UNIQUE INDEX idx_top_k_queries_per_day \n",
    "    ON public.top_k_queries_per_day (day, query_type, user_id);\n",
    "\n",
    "    -- Refresh the materialized view concurrently\n",
    "    REFRESH MATERIALIZED VIEW CONCURRENTLY public.top_k_queries_per_day;\n",
    "    SELECT * FROM public.top_k_queries_per_day LIMIT 10;\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(create_view_query))\n",
    "        conn.commit()\n",
    "    print(\"✅ Ensured 'top_k_queries_per_day' materialized view exists.\")\n",
    "\n",
    "def compile_time_vs_num_joins_materialized_view():\n",
    "    # First part: Drop and create the materialized view inside a transaction block\n",
    "    create_view_query = \"\"\"\n",
    "    -- DROP MATERIALIZED VIEW IF EXISTS compile_time_vs_num_joins CASCADE;\n",
    "\n",
    "    CREATE MATERIALIZED VIEW public.compile_time_vs_num_joins AS\n",
    "    SELECT \n",
    "        num_joins AS x,\n",
    "        AVG(compile_duration_ms) AS y\n",
    "    FROM public.redset_main\n",
    "    WHERE query_type = 'select' \n",
    "    AND num_joins IS NOT NULL\n",
    "    GROUP BY num_joins\n",
    "    ORDER BY num_joins with DATA;\n",
    "    \"\"\"\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        try:\n",
    "            # Start transaction block\n",
    "            conn.execute(text('BEGIN'))\n",
    "\n",
    "            # Execute the materialized view creation\n",
    "            conn.execute(text(create_view_query))\n",
    "\n",
    "            # Commit the transaction block\n",
    "            conn.execute(text('COMMIT'))\n",
    "\n",
    "            print(\"✅ Materialized view 'compile_time_vs_num_joins' created successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Rollback in case of error\n",
    "            conn.execute(text('ROLLBACK'))\n",
    "            print(f\"❌ Error creating materialized view: {e}\")\n",
    "    \n",
    "    # Second part: Create the index outside the transaction block using autocommit\n",
    "    create_index_query = \"\"\"\n",
    "    CREATE UNIQUE INDEX CONCURRENTLY idx_compile_time_vs_num_joins \n",
    "    ON public.compile_time_vs_num_joins (x, y);\n",
    "    \"\"\"\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        try:\n",
    "            # Set autocommit to True to bypass transaction block\n",
    "            conn.execution_options(isolation_level=\"AUTOCOMMIT\").execute(text(create_index_query))\n",
    "\n",
    "            print(\"✅ Unique index 'idx_compile_time_vs_num_joins' created successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error creating index: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "ensure_master_table()\n",
    "ensure_anomalies_table()\n",
    "top_k_tables_per_day_materialized_view()\n",
    "top_k_queries_per_day_materialized_view()\n",
    "hit_rate_per_day_materialized_view()\n",
    "compile_time_vs_num_joins_materialized_view()\n",
    "# stream_summary_table()\n",
    "# global_counts_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to insert data into anomalies table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_batch(data):\n",
    "    \"\"\"\n",
    "    Validates a batch of query logs stored in a pandas DataFrame.\n",
    "    Stores anomalies in the PostgreSQL 'anomalies' table (one row per input row).\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"No data to validate.\")\n",
    "        return\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Convert data to expected schema\n",
    "    df[\"arrival_timestamp\"] = pd.to_datetime(df[\"arrival_timestamp\"], errors=\"coerce\")\n",
    "    df[\"compile_duration_ms\"] = pd.to_numeric(df[\"compile_duration_ms\"], errors=\"coerce\")\n",
    "    df[\"execution_duration_ms\"] = pd.to_numeric(df[\"execution_duration_ms\"], errors=\"coerce\")\n",
    "    df[\"was_cached\"] = pd.to_numeric(df[\"was_cached\"], errors=\"coerce\")\n",
    "    df[\"query_type\"] = df[\"query_type\"].astype(str)\n",
    "    df[\"read_table_ids\"] = df[\"read_table_ids\"].astype(str)\n",
    "    df[\"write_table_ids\"] = df[\"write_table_ids\"].astype(str)\n",
    "    df[\"num_joins\"] = pd.to_numeric(df[\"num_joins\"], errors=\"coerce\")\n",
    "\n",
    "    # Store anomalies per row\n",
    "    anomalies_per_row = {}\n",
    "\n",
    "    def add_anomaly(index, message):\n",
    "        \"\"\" Helper function to accumulate anomalies for each row \"\"\"\n",
    "        if index not in anomalies_per_row:\n",
    "            anomalies_per_row[index] = []\n",
    "        anomalies_per_row[index].append(message)\n",
    "\n",
    "    # Type Validation (Ignore Trivial Differences)\n",
    "    for column, expected_dtype in expected_schema.items():\n",
    "        if column in df.columns:\n",
    "            for idx, value in df[column].items():\n",
    "                if pd.isna(value):\n",
    "                    continue  # Skip NaN values\n",
    "                \n",
    "                actual_type = type(value).__name__\n",
    "\n",
    "                # Allow compatible types\n",
    "                if expected_dtype == \"int\" and isinstance(value, int):\n",
    "                    continue\n",
    "                if expected_dtype == \"float\" and isinstance(value, (float, int)):  # Allow int for float fields\n",
    "                    continue\n",
    "                if expected_dtype == \"bool\" and isinstance(value, bool):\n",
    "                    continue\n",
    "                if expected_dtype == \"datetime64[ns]\" and isinstance(value, pd.Timestamp):\n",
    "                    continue\n",
    "                if expected_dtype == \"str\" and isinstance(value, str):\n",
    "                    continue\n",
    "\n",
    "                # If mismatch remains, record anomaly\n",
    "                add_anomaly(idx, f\"{column} expected {expected_dtype}, found {actual_type}\")\n",
    "\n",
    "    # Validate `query_type`\n",
    "    invalid_query_types = df.loc[~df[\"query_type\"].isin(valid_query_types)]\n",
    "    for idx, row in invalid_query_types.iterrows():\n",
    "        add_anomaly(idx, f\"Invalid query_type `{row['query_type']}`\")\n",
    "\n",
    "    # Check Missing Table IDs\n",
    "    missing_read_ids = df.loc[(df[\"query_type\"].isin({\"select\", \"copy\"})) & df[\"read_table_ids\"].isna()]\n",
    "    for idx, row in missing_read_ids.iterrows():\n",
    "        add_anomaly(idx, f\"Missing read_table_ids for `{row['query_type']}` query\")\n",
    "\n",
    "    missing_write_ids = df.loc[(df[\"query_type\"].isin({\"insert\", \"delete\", \"copy\"})) & df[\"write_table_ids\"].isna()]\n",
    "    for idx, row in missing_write_ids.iterrows():\n",
    "        add_anomaly(idx, f\"Missing write_table_ids for `{row['query_type']}` query\")\n",
    "\n",
    "    # Check Invalid Table IDs\n",
    "    df[\"read_table_ids\"] = df[\"read_table_ids\"].fillna(\"\").astype(str)\n",
    "    df[\"write_table_ids\"] = df[\"write_table_ids\"].fillna(\"\").astype(str)\n",
    "\n",
    "    invalid_read_ids = df[\"read_table_ids\"].str.split(\",\").apply(set).apply(lambda x: x & invalid_table_ids)\n",
    "    for idx, invalid_vals in invalid_read_ids.items():\n",
    "        if invalid_vals:\n",
    "            add_anomaly(idx, f\"read_table_ids contains invalid values {invalid_vals}\")\n",
    "\n",
    "    invalid_write_ids = df[\"write_table_ids\"].str.split(\",\").apply(set).apply(lambda x: x & invalid_table_ids)\n",
    "    for idx, invalid_vals in invalid_write_ids.items():\n",
    "        if invalid_vals:\n",
    "            add_anomaly(idx, f\"write_table_ids contains invalid values {invalid_vals}\")\n",
    "\n",
    "    # Execution duration check\n",
    "    execution_issues = df.loc[df[\"execution_duration_ms\"] < 0]\n",
    "    for idx, row in execution_issues.iterrows():\n",
    "        add_anomaly(idx, f\"execution_duration_ms is negative ({row['execution_duration_ms']})\")\n",
    "\n",
    "    # Queries with execution_duration_ms == 0 but was_cached != 1\n",
    "    cache_issues = df.loc[(df[\"execution_duration_ms\"] == 0) & (df[\"was_cached\"] != 1)]\n",
    "    for idx, row in cache_issues.iterrows():\n",
    "        add_anomaly(idx, f\"Query executed instantly but was not cached\")\n",
    "\n",
    "    # Insert anomalies into PostgreSQL\n",
    "    if anomalies_per_row:\n",
    "        anomalies_df = pd.DataFrame({\n",
    "            \"arrival_timestamp\": df.loc[list(anomalies_per_row.keys()), \"arrival_timestamp\"].values,\n",
    "            \"query_id\": df.loc[list(anomalies_per_row.keys()), \"query_id\"].values,  # Added query_id to DataFrame\n",
    "            \"anomaly_description\": [\" | \".join(issues) for issues in anomalies_per_row.values()]\n",
    "        })\n",
    "        anomalies_df.to_sql(\"anomalies\", engine, if_exists=\"append\", index=False)\n",
    "        print(f\"✅ {len(anomalies_df)} anomalies inserted into PostgreSQL.\")\n",
    "    else:\n",
    "        pass\n",
    "        # print(\"✅ No anomalies found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to insert data into redset_main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert a batch of rows into the master table\n",
    "prev_date = None\n",
    "\n",
    "def insert_batch_into_master(data):\n",
    "    \"\"\"\n",
    "    Inserts a batch of JSON-formatted rows into the PostgreSQL redset_main table,\n",
    "    distributing rows into their respective date partitions.\n",
    "\n",
    "    Parameters:\n",
    "        data (list of dicts): A list containing N rows as dictionaries.\n",
    "    \"\"\"\n",
    "    global prev_date\n",
    "    \n",
    "    if not data:\n",
    "        print(\"No data to insert.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Convert data into a Pandas DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Convert 'arrival_timestamp' column to datetime\n",
    "        df['arrival_timestamp'] = pd.to_datetime(df['arrival_timestamp'])\n",
    "\n",
    "        # Extract the date from 'arrival_timestamp'\n",
    "        df['date_partition'] = df['arrival_timestamp'].dt.date\n",
    "\n",
    "        # # Convert 'was_cached' column to boolean (True/False)\n",
    "        # if 'was_cached' in df.columns:\n",
    "        #     df['was_cached'] = df['was_cached'].astype(bool)\n",
    "\n",
    "        # Handle NaN values (replace NaN with None for PostgreSQL compatibility)\n",
    "        df = df.where(pd.notna(df), None)\n",
    "\n",
    "        insert_start_time = time.time()\n",
    "\n",
    "        # Group rows by date_partition and insert into respective partitions\n",
    "        for partition_date, partition_df in df.groupby('date_partition'):\n",
    "            with engine.connect() as conn:\n",
    "                partition_name = f'redset_{partition_date.strftime(\"%Y_%m_%d\")}'  # Example: Master_2024_03_01\n",
    "                \n",
    "                # Ensure the partition exists (optional step, otherwise assume it's pre-created)\n",
    "                create_partition_query = f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS public.\"{partition_name}\"\n",
    "                PARTITION OF public.\"redset_main\"\n",
    "                FOR VALUES FROM ('{partition_date}') TO ('{partition_date + pd.Timedelta(days=1)}');\n",
    "                \"\"\"\n",
    "\n",
    "                if partition_date != prev_date:\n",
    "                    # Delete old partition\n",
    "                    partition_to_delete = f'redset_{(partition_date - timedelta(days=4)).strftime(\"%Y_%m_%d\")}'\n",
    "                    delete_partition_query = f\"\"\"\n",
    "                        DROP TABLE IF EXISTS public.\"{partition_to_delete}\";\n",
    "                    \"\"\"\n",
    "\n",
    "                    # Refresh materialized views\n",
    "                    refresh_queries = [\n",
    "                        \"REFRESH MATERIALIZED VIEW CONCURRENTLY public.top_k_tables_per_day;\" ,\n",
    "                        \"REFRESH MATERIALIZED VIEW CONCURRENTLY public.top_k_queries_per_day;\",\n",
    "                        \"REFRESH MATERIALIZED VIEW CONCURRENTLY public.hit_rate_per_day;\"\n",
    "                        \"REFRESH MATERIALIZED VIEW CONCURRENTLY public.compile_time_vs_num_joins;\"\n",
    "                    ]\n",
    "\n",
    "                    conn.execute(text(delete_partition_query))\n",
    "                    for query in refresh_queries:\n",
    "                        conn.execute(text(query))\n",
    "\n",
    "                # Run all the queries\n",
    "                conn.execute(text(create_partition_query))  # Wrap SQL in text()\n",
    "                \n",
    "                conn.commit()  # Ensure queries run\n",
    "\n",
    "                # Insert into the respective partition\n",
    "                partition_df.drop(columns=['date_partition'], inplace=True)  # Remove helper column\n",
    "                partition_df.to_sql(partition_name, engine, if_exists='append', index=False)\n",
    "\n",
    "                prev_date = partition_date\n",
    "\n",
    "        # print(f\"✅ Inserted {len(partition_df)} rows into partition {partition_name}\")\n",
    "\n",
    "        insert_time = time.time() - insert_start_time\n",
    "        # print(f\"✅ Total insert time: {insert_time:.2f} seconds\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error inserting data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main consumer loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer running...\n",
      "No message received during poll. Retrying...\n",
      "No message received during poll. Retrying...\n",
      "No message received during poll. Retrying...\n",
      "No message received during poll. Retrying...\n",
      "No message received during poll. Retrying...\n",
      "No message received during poll. Retrying...\n",
      "No message received during poll. Retrying...\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "✅ 2 anomalies inserted into PostgreSQL.\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "❌ Error inserting data: (psycopg2.OperationalError) server closed the connection unexpectedly\n",
      "\tThis probably means the server terminated abnormally\n",
      "\tbefore or while processing the request.\n",
      "\n",
      "[SQL: \n",
      "                CREATE TABLE IF NOT EXISTS public.\"redset_2024_03_01\"\n",
      "                PARTITION OF public.\"redset_main\"\n",
      "                FOR VALUES FROM ('2024-03-01') TO ('2024-03-02');\n",
      "                ]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "Consumer error: KafkaError{code=_MAX_POLL_EXCEEDED,val=-147,str=\"Application maximum poll interval (300000ms) exceeded by 363ms\"}\n",
      "❌ Error inserting data: (psycopg2.OperationalError) server closed the connection unexpectedly\n",
      "\tThis probably means the server terminated abnormally\n",
      "\tbefore or while processing the request.\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "Consumer error: KafkaError{code=_MAX_POLL_EXCEEDED,val=-147,str=\"Application maximum poll interval (300000ms) exceeded by 245ms\"}\n",
      "✅ 2 anomalies inserted into PostgreSQL.\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "❌ Error inserting data: (psycopg2.OperationalError) server closed the connection unexpectedly\n",
      "\tThis probably means the server terminated abnormally\n",
      "\tbefore or while processing the request.\n",
      "\n",
      "[SQL: \n",
      "                CREATE TABLE IF NOT EXISTS public.\"redset_2024_03_01\"\n",
      "                PARTITION OF public.\"redset_main\"\n",
      "                FOR VALUES FROM ('2024-03-01') TO ('2024-03-02');\n",
      "                ]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "❌ Error inserting data: (psycopg2.OperationalError) connection to server at \"192.168.7.138\", port 5432 failed: Connection timed out (0x0000274C/10060)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "❌ Error inserting data: (psycopg2.OperationalError) connection to server at \"192.168.7.138\", port 5432 failed: Connection timed out (0x0000274C/10060)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "✅ 1 anomalies inserted into PostgreSQL.\n",
      "❌ Error inserting data: (psycopg2.OperationalError) server closed the connection unexpectedly\n",
      "\tThis probably means the server terminated abnormally\n",
      "\tbefore or while processing the request.\n",
      "\n",
      "[SQL: \n",
      "                CREATE TABLE IF NOT EXISTS public.\"redset_2024_03_01\"\n",
      "                PARTITION OF public.\"redset_main\"\n",
      "                FOR VALUES FROM ('2024-03-01') TO ('2024-03-02');\n",
      "                ]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "❌ Error inserting data: (psycopg2.OperationalError) connection to server at \"192.168.7.138\", port 5432 failed: Network is unreachable (0x00002743/10051)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "❌ Error inserting data: (psycopg2.OperationalError) connection to server at \"192.168.7.138\", port 5432 failed: Network is unreachable (0x00002743/10051)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "❌ Error inserting data: (psycopg2.OperationalError) connection to server at \"192.168.7.138\", port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "❌ Error inserting data: (psycopg2.OperationalError) connection to server at \"192.168.7.138\", port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "❌ Error inserting data: (psycopg2.OperationalError) connection to server at \"192.168.7.138\", port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "❌ Error inserting data: (psycopg2.OperationalError) connection to server at \"192.168.7.138\", port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "❌ Error inserting data: (psycopg2.OperationalError) connection to server at \"192.168.7.138\", port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "❌ Error inserting data: (psycopg2.OperationalError) connection to server at \"192.168.7.138\", port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n"
     ]
    }
   ],
   "source": [
    "# Kafka host and topic to match the producer\n",
    "kafka_host = \"localhost:9092\"\n",
    "kafka_topic = \"muhid\"  # Topic where messages are produced\n",
    "\n",
    "# Initialize the consumer with the correct configuration\n",
    "c = Consumer({\n",
    "    'bootstrap.servers': kafka_host,\n",
    "    'group.id': 'redset_stream',  # Static group ID\n",
    "    'auto.offset.reset': 'earliest',  # Start reading from the earliest available message\n",
    "    'debug': 'all',  # Enable all Kafka debugging logs\n",
    "    'fetch.message.max.bytes': 104857600,  # 100MB\n",
    "    'max.partition.fetch.bytes': 104857600,\n",
    "    'session.timeout.ms': 60000,\n",
    "})\n",
    "\n",
    "c.subscribe([kafka_topic])\n",
    "\n",
    "# Variables to calculate average\n",
    "tmp = 20\n",
    "v = 0\n",
    "prev_data = 0\n",
    "\n",
    "msg_printed = 0\n",
    "file_writer = 0\n",
    "\n",
    "print(\"Consumer running...\")\n",
    "while True:\n",
    "    msg = c.poll(3.0)\n",
    "    time.sleep(2.0)\n",
    "    if msg is None:\n",
    "        print('No message received during poll. Retrying...')\n",
    "        continue\n",
    "    if msg.error():\n",
    "        print(f\"Consumer error: {msg.error()}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Decode the message\n",
    "        message = msg.value().decode('utf-8')\n",
    "        # print(f\"Received message: {message}\")\n",
    "        \n",
    "        # Parse JSON\n",
    "        message_data = json.loads(message)\n",
    "        if prev_data == message_data:\n",
    "            # print(f\"Prev data same\")\n",
    "            pass\n",
    "        else:\n",
    "            # print(f\"Prev data NOT same!!!\")\n",
    "            pass\n",
    "        \n",
    "        prev_data = deepcopy(message_data)\n",
    "\n",
    "        validate_start_time = time.time()\n",
    "        # Check for anomalies and insert into anomaly table\n",
    "        validate_batch(message_data)\n",
    "        validate_time = time.time() - validate_start_time\n",
    "\n",
    "        # Insert the batch into PostgreSQL\n",
    "        insert_batch_into_master(message_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing message: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"Consumer stopped.\")\n",
    "# Clean up\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
