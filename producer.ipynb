{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL - Pyarrow stream batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to stream Parquet file: ../sorted_p.parquet\n",
      "Total rows in Parquet file: 433107811\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import confluent_kafka\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Custom JSON encoder to handle Timestamp\n",
    "def json_serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f'Object of type {obj.__class__.__name__} is not JSON serializable')\n",
    "\n",
    "def stream_parquet_to_kafka(parquet_file_path, kafka_topic, bootstrap_servers):\n",
    "    # Start total time tracking\n",
    "    total_start_time = time.time()\n",
    "    print(f\"Starting to stream Parquet file: {parquet_file_path}\")\n",
    "\n",
    "    # Configure Kafka producer\n",
    "    producer_config = {\n",
    "        'bootstrap.servers': bootstrap_servers,\n",
    "        'client.id': 'parquet-streamer',\n",
    "        'batch.size': 50000000,       # Increase batch size (50MB)\n",
    "        'linger.ms': 100,              # Small delay for better batching\n",
    "        'compression.type': 'gzip',    # Compress messages\n",
    "        'message.max.bytes': 104857600, # 100MB\n",
    "        'queue.buffering.max.messages': 1000000, # Handle large queues\n",
    "    }\n",
    "    producer = confluent_kafka.Producer(producer_config)\n",
    "\n",
    "    # Open Parquet file and get file metadata\n",
    "    parquet_file = pq.ParquetFile(parquet_file_path)\n",
    "    total_rows = parquet_file.metadata.num_rows\n",
    "    print(f\"Total rows in Parquet file: {total_rows}\")\n",
    "\n",
    "    # Tracking variables\n",
    "    total_rows_processed = 0\n",
    "    total_batch_time = 0\n",
    "    total_kafka_time = 0\n",
    "    file_writer = 0\n",
    "\n",
    "    # Variable to track rows processed per day\n",
    "    rows_processed_today = 0\n",
    "    max_rows_per_day = 45000  # Limit per day\n",
    "\n",
    "    # Variable to track the current date\n",
    "    current_day = None\n",
    "\n",
    "    # Iterate through batches instead of individual rows\n",
    "    for batch_num, batch in enumerate(parquet_file.iter_batches(batch_size = 800), 1):\n",
    "\n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        # Convert batch to pandas for easier processing\n",
    "        df_batch = batch.to_pandas()\n",
    "        batch_conversion_time = time.time() - batch_start_time\n",
    "\n",
    "        df_batch = df_batch.drop(columns = [\"mbytes_scanned\", \"instance_id\", \"cluster_size\", \"database_id\", \"queue_duration_ms\", \"feature_fingerprint\", \"was_aborted\", \"cache_source_query_id\", \"num_permanent_tables_accessed\", \"num_external_tables_accessed\", \"num_system_tables_accessed\", \"mbytes_spilled\", \"num_aggregations\", \"rn\"])\n",
    "\n",
    "        # Convert entire batch to records and stream\n",
    "        kafka_start_time = time.time()\n",
    "        # Convert entire batch to JSON and stream as a single message\n",
    "        batch_messages = json.dumps(df_batch.to_dict('records'), default=json_serializer)\n",
    "        producer.produce(\n",
    "            topic=kafka_topic, \n",
    "            value=batch_messages.encode('utf-8')\n",
    "        )\n",
    "\n",
    "        # Flush to ensure messages are sent\n",
    "        producer.flush()\n",
    "        \n",
    "        kafka_time = time.time() - kafka_start_time\n",
    "        total_kafka_time += kafka_time\n",
    "\n",
    "        total_rows_processed += len(df_batch)\n",
    "        batch_total_time = time.time() - batch_start_time\n",
    "        total_batch_time += batch_total_time\n",
    "        \n",
    "    # Close producer\n",
    "    producer.close()\n",
    "\n",
    "    # Print final statistics\n",
    "    total_runtime = time.time() - total_start_time\n",
    "    print(\"\\n--- Streaming Complete ---\")\n",
    "    print(f\"Total runtime: {total_runtime:.2f} seconds\")\n",
    "    print(f\"Total rows processed: {total_rows_processed}\")\n",
    "    print(f\"Total Kafka production time: {total_kafka_time:.2f} seconds\")\n",
    "    print(f\"Average batch processing time: {total_batch_time / batch_num:.2f} seconds\")\n",
    "\n",
    "# Example usage\n",
    "stream_parquet_to_kafka(\n",
    "    parquet_file_path='../sorted_p.parquet', \n",
    "    kafka_topic='muhid', \n",
    "    bootstrap_servers='localhost:9092'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
